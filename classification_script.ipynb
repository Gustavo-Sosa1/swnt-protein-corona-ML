{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# After Using Previous Notebooks Use This Notebook to Make Predictions"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Imports --- All of this may not be vital\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import log_loss, f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from imblearn.over_sampling import *\n",
    "\n",
    "# Homemade functions required\n",
    "from data_prep_functions import *\n",
    "from interpro_scraping import interpro_scraping_pandas"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Bio'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 25>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MinMaxScaler\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# from imblearn.over_sampling import *\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# Homemade functions required\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdata_prep_functions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minterpro_scraping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interpro_scraping_pandas\n",
      "File \u001B[1;32m~\\PycharmProjects\\swnt-protein-corona-ML\\data_prep_functions.py:6\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minterpro_scraping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m interpro_scraping_pandas\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mBio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mSeqUtils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mProtParam\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ProteinAnalysis\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'Bio'"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Data"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "source": [
    "### import data used to train classifiers ###\n",
    "\n",
    "plasma_total_data_names = pd.read_excel(\"data/\"+'gt15_plasma_features_names_biopy_gravy.xlsx', header=0, index_col=0)\n",
    "# gt6_data = pd.read_excel(\"data/\"+'gt6_plasma_features_names_biopy.xlsx', header=0, index_col=0)\n",
    "csf_total_data_names = pd.read_excel(\"data/\"+'gt15_csf_features_names_biopy_gravy.xlsx', header=0,index_col=0)\n",
    "\n",
    "\n",
    "## sort into names and features\n",
    "features_plasma = plasma_total_data_names.copy()\n",
    "features_plasma = features_plasma.drop(['Corona'], axis=1)\n",
    "names_plasma = plasma_total_data_names['Corona'].copy()\n",
    "\n",
    "features_csf = csf_total_data_names.copy()\n",
    "features_csf = features_csf.drop(['Corona'], axis=1) \n",
    "names_csf = csf_total_data_names['Corona'].copy()\n",
    "\n",
    "### create a merged set\n",
    "features_plasma_labeled = features_plasma.copy()\n",
    "features_csf_labeled = features_csf.copy()\n",
    "\n",
    "features_plasma_labeled['phase_plasma'] = 1\n",
    "features_csf_labeled['phase_plasma'] = 0\n",
    "\n",
    "features_merged = features_plasma_labeled.append(features_csf_labeled, ignore_index=True)\n",
    "names_merged = names_plasma.append(names_csf, ignore_index=True)\n",
    "\n",
    "# set with no phase labeling names are identical to names merged\n",
    "features_merged_naive = features_merged.drop(['phase_plasma'], axis=1)\n",
    "\n",
    "# print(plasma_total_data_names.shape, csf_total_data_names.shape, features_test.shape) ## in case you need to see shapes\n",
    "\n",
    "\n",
    "## there is a known error here, sometimes there is an Unnamed column just drop it code is available in a \n",
    "#lower cell (scaling cell), its a holdover from two merged set\n",
    "\n",
    "# tf_data = features_merged_naive.copy()\n",
    "# tf_data['names'] = names_merged.copy()\n",
    "# tf_data.to_excel('data_for_tensorflow.xlsx')\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "#### revisions \n",
    "\n",
    "total_data_for_reductions = features_merged_naive.copy()\n",
    "total_data_for_reductions['Corona'] = names_merged\n",
    "\n",
    "total_data_reduced = total_data_for_reductions.drop_duplicates(subset=['Protein names'])\n",
    "total_data_reduced.shape\n",
    "\n",
    "names_reduced = total_data_reduced['Corona']\n",
    "features_reduced = total_data_reduced.drop('Corona', axis=1)\n",
    "reduced_protein_names = features_reduced['Protein names']"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "source": [
    "## Use this space to import test data ### \n",
    "\n",
    "\n",
    "features_test = pd.read_excel(\"data/\"+'proteins_selected_for_testing_complete_updated.xlsx', header=0, index_col = 0)\n",
    "\n",
    "# uncomment below for large verification runs with labels\n",
    "\n",
    "# features_test = pd.read_excel(\"data/\"+'pnp_csf_features_names_biopy_gravy.xlsx', header=0, index_col = 0)\n",
    "# y_test_test = features_test['Corona'].copy()\n",
    "# features_test = features_test.drop(['Corona'], axis=1)\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "### if were not going to be using NETSURFP \n",
    "\n",
    "features_for_prediction = pd.read_excel(\"data/\"+'proteins_selected_for_testing_complete.xlsx', header=0, index_col = 0)\n",
    "# print(list(features_for_prediction.columns))\n",
    "# subset_features = features_merged_naive[list(features_for_prediction.columns)]"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scale Data To Make it Work Well"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "source": [
    "scaler = MinMaxScaler()\n",
    "total_data = features_merged_naive.copy()  ## for a regular netsurfp included case\n",
    "# total_data = subset_features.copy() ### for a subset case --- use this one\n",
    "total_data = total_data.fillna(0)\n",
    "total_data_with_names = total_data.copy()\n",
    "total_data = total_data.drop(['Protein names', 'mass' ], axis=1)\n",
    "scaler = scaler.fit(total_data)\n",
    "scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "print(scaled_df.shape)\n",
    "\n",
    "\n",
    "scaled_df_phase = scaled_df.copy()\n",
    "scaled_df_phase['phase_plasma'] = features_merged['phase_plasma'].copy()\n",
    "\n",
    "plasma_data = scaled_df_phase[scaled_df_phase.phase_plasma==1]\n",
    "plasma_data = plasma_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_plasma = plasma_data #pd.DataFrame(scaler.transform(plasma_data), columns=plasma_data.columns)\n",
    "\n",
    "csf_data = scaled_df_phase[scaled_df_phase.phase_plasma==0]\n",
    "csf_data = csf_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_csf = csf_data #pd.DataFrame(scaler.transform(csf_data), columns=csf_data.columns)\n",
    "\n",
    "### UNCOMMENT this section for a REGULAR RUN\n",
    "#features = features_merged_naive.copy()  # change the dataframe that you want to use here\n",
    "features_test = features_test.fillna(0)\n",
    "features_test_names = features_test.copy()\n",
    "features_test = features_test.drop(['Protein names', 'mass'], axis=1) #,'entry'\n",
    "scaled_test_df = pd.DataFrame(scaler.transform(features_test), columns=features_test.columns)\n",
    "\n",
    "# features_reduced = features_reduced.drop(['Protein names'], axis=1)\n",
    "# scaled_reduced_df = pd.DataFrame(scaler.transform(features_reduced), columns=features_reduced.columns)\n",
    "\n",
    "\n",
    "# scaled_df = scaled_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "# scaled_df_phase = scaled_df_phase.drop(['Unnamed: 0.1'], axis=1)\n",
    "# scaled_test_df = scaled_test_df.drop(['Unnamed: 0.1'], axis=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(174, 91)\n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "gt6_data = pd.read_excel(\"data/\"+'pnp_plasma_features_names_biopy_gravy.xlsx', header=0, index_col=0)\n",
    "features_gt6 = gt6_data.copy()\n",
    "features_gt6 = features_gt6.drop(['Corona'], axis=1)\n",
    "names_gt6 = gt6_data['Corona'].copy()\n",
    "\n",
    "features_gt6_combined = features_merged_naive.append(features_gt6, ignore_index=True)\n",
    "names_gt6_combined = names_merged.append(names_gt6, ignore_index=True)\n",
    "features_gt6_combined = features_gt6_combined.drop(columns=['Protein names', 'mass'])\n",
    "total_data_col_drop = total_data.copy()\n",
    "\n",
    "# scaler = scaler.fit(features_gt6_combined)\n",
    "scaled_df_gt6 = pd.DataFrame(scaler.transform(features_gt6_combined), columns=total_data_col_drop.columns)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data to be Put into classifier"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "source": [
    "### Data put into classifier and classified\n",
    "s_df = scaled_df#.drop(['Unnamed: 0.1', 'mass' ], axis=1)\n",
    "scaled_test_df = scaled_test_df#.drop(['Unnamed: 0.1', 'mass'], axis=1)\n",
    "\n",
    "df_local_features_train = s_df.copy()\n",
    "# df_local_protein_names_train = features_merged['Protein names'].copy()#reduced_protein_names.copy()\n",
    "df_local_names = names_merged.copy()\n",
    "\n",
    "df_local_features_classify = scaled_test_df.copy() #.drop(['Unnamed: 0.1'], axis=1)\n",
    "df_local_protein_names_classify = features_test_names['Protein names'] #.copy()\n",
    "\n",
    "### to keep some things kosher later\n",
    "df_local_features_train_copy = s_df.copy()\n",
    "df_local_names_copy = names_merged.copy()\n",
    "df_local_features_classify_copy = df_local_features_classify.copy()\n",
    "df_local_protein_names_classify_copy = df_local_protein_names_classify.copy()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "source": [
    "k_fold_splits = 100\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "X_new = SelectKBest(f_classif, k=38).fit_transform(df_local_features_train.copy(), df_local_names.copy()) #X_train_set.values #\n",
    "df_local_features_train = pd.DataFrame(X_new.copy())#df_local_features_train.copy() #pd.DataFrame(X_new.copy()) #scaled_df.copy()\n",
    "\n",
    "rndm_ste=2016\n",
    "feature_imp = pd.DataFrame(columns=list(df_local_features_train.columns))\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "set_size_adjust = (scaled_df.shape[0]/scaled_df_plasma.shape[0]) *.1 # used to retain the same number of samples in the test set, replace test_size with it if using\n",
    "# #split up our data\n",
    "i = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=k_fold_splits, test_size=0.1, random_state=rndm_ste)\n",
    "\n",
    "for train_index, test_index in sss.split(df_local_features_train, df_local_names): # comment our if doing cross fluid\n",
    "# for train_index, test_index in sss.split(scaled_df_plasma, names_plasma): # use for cross fluid tests, verify correct dataset placed here\n",
    "    X_train = df_local_features_train.iloc[train_index] # remove subsetting for cross fluid tests\n",
    "    X_test = df_local_features_train.iloc[test_index] # change dataframe for cross fluid tests\n",
    "    y_train = df_local_names.iloc[train_index] # remove subsetting for cross fluid tests\n",
    "    y_test = df_local_names.iloc[test_index] # change dataframe for cross fluid tests\n",
    "\n",
    "\n",
    "     \n",
    "    # Create and Train\n",
    "    rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "     n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700) \n",
    " \n",
    "    \n",
    "    sme = SMOTE(random_state=rndm_ste, sampling_strategy=0.7, n_jobs=-1, k_neighbors=12)\n",
    "    X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "    # X_train_oversampled, y_train_oversampled = X_train, y_train # can be used to pass smote if needed for an experiment\n",
    "    rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if first_frame:  # Initialize \n",
    "        first_frame = False  # Don't Come back Here\n",
    "        \n",
    "        datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "        \n",
    "        correctness_frame = pd.DataFrame(data=datadict)\n",
    "        correctness_frame['round'] = i\n",
    "\n",
    "        metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)), \n",
    "        'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "        \n",
    "        metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "        metrics_frame['Round'] = i\n",
    "\n",
    "        # can be used if you want to track prediction during shuffle split - saves in another cell\n",
    "        predictions = pd.DataFrame()\n",
    "        predictions['Protein Name'] = df_local_protein_names_classify\n",
    "        predictions['In Corona Probability'] = rfc.predict_proba(df_local_features_classify)[:, 1]\n",
    "        predictions['Round'] = i\n",
    "        predictions['Test Accuracy'] = metrics_dict['Accuracy']\n",
    "        predictions['Test Recall'] = metrics_dict['Recall']\n",
    "        predictions['Test Precision'] = metrics_dict['Precision']\n",
    "        predictions['Test AUC'] = metrics_dict['AUC']\n",
    "\n",
    "        \n",
    "    else:\n",
    "        datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "        revolve_frame = pd.DataFrame(data=datadict)\n",
    "        revolve_frame['round'] = i\n",
    "        correctness_frame = correctness_frame.append(revolve_frame, ignore_index=True)\n",
    "\n",
    "        metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)), \n",
    "        'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "        metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "        metrics_revolve_frame['Round'] = i\n",
    "        metrics_frame = metrics_frame.append(metrics_revolve_frame, ignore_index=True)\n",
    "\n",
    "        # can be used if you want to track prediction during shuffle split - saves in another cell\n",
    "        pred_rev = pd.DataFrame()\n",
    "        pred_rev['Protein Name'] = df_local_protein_names_classify\n",
    "        pred_rev['In Corona Probability'] = rfc.predict_proba(df_local_features_classify)[:, 1]\n",
    "        pred_rev['Round'] = i\n",
    "        pred_rev['Test Accuracy'] = metrics_dict['Accuracy']\n",
    "        pred_rev['Test Recall'] = metrics_dict['Recall']\n",
    "        pred_rev['Test Precision'] = metrics_dict['Precision']\n",
    "        pred_rev['Test AUC'] = metrics_dict['AUC']\n",
    "\n",
    "        predictions = predictions.append(pred_rev, ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "    feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df_local_features_train.columns))\n",
    "    \n",
    "    i += 1"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "source": [
    "# displays results\n",
    "metrics_frame.mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AUC           0.757708\n",
       "Accuracy      0.775556\n",
       "Recall        0.646667\n",
       "Precision     0.694506\n",
       "F1            0.646936\n",
       "Round        49.500000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Collect Revolving Predictions (if applicable)\n",
    "\n",
    "This is a feature that is not used in the manuscript "
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unique_names = []\n",
    "for i in predictions['Protein Name']:\n",
    "    if i not in unique_names:\n",
    "        unique_names.append(i)\n",
    "\n",
    "protein_avg_predictions = pd.DataFrame()\n",
    "counter = 0\n",
    "for i in unique_names:\n",
    "    avg_df = predictions[predictions['Protein Name'] == i]\n",
    "\n",
    "    if counter == 0:\n",
    "        protein_avg_predictions = pd.DataFrame([i, round(avg_df['In Corona Probability'].mean(), 3), round(confidence_interval(avg_df['In Corona Probability']), 3)], index=['Protein Name', 'Average In Corona Probability', '95 Percent Confidence Interval']).transpose()\n",
    "        \n",
    "    else:\n",
    "        pap_df = pd.DataFrame([i, round(avg_df['In Corona Probability'].mean(), 3), round(confidence_interval(avg_df['In Corona Probability']), 3)], index=['Protein Name', 'Average In Corona Probability', '95 Percent Confidence Interval']).transpose()\n",
    "        protein_avg_predictions = protein_avg_predictions.append(pap_df, ignore_index=True)\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "protein_avg_predictions"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pct_correct = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    subset = correctness_frame[correctness_frame.probability>= i *.1]\n",
    "    subset = subset[subset.probability <(i+1)*.1]\n",
    "\n",
    "#     subset = correctness_frame[correctness_frame.probability>=i]\n",
    "    subset['correct'] = subset['true'] == subset['estimate']\n",
    "    pct_correct.append(subset.correct.sum() / subset.shape[0])\n",
    "    \n",
    "bar_names = ['[' + str(np.around((i-1)*.1, decimals=1)) + ', ' + str(np.around((i)*.1, decimals=1)) +')'  for i in range(1,11)]\n",
    "print(pct_correct, bar_names)#, steps)\n",
    "#subset\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=bar_names, y=pct_correct, ci=None)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "overall_probability_accuracy = pd.DataFrame([pct_correct], columns=bar_names)\n",
    "metrics_frame = metrics_frame.append(pd.DataFrame({\"AUC\":[metrics_frame.AUC.mean(), confidence_interval(metrics_frame.AUC)], \"Accuracy\":[metrics_frame.Accuracy.mean(), confidence_interval(metrics_frame.Accuracy)], \"Precision\":[metrics_frame.Precision.mean(), confidence_interval(metrics_frame.Precision)],'Round':['Average', '.95 CI'], 'Recall':[metrics_frame.Recall.mean(), confidence_interval(metrics_frame.Recall)], 'F1':[metrics_frame.F1.mean(), confidence_interval(metrics_frame.F1)]}), ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Predictions Using the Entire Saved Dataset\n",
    "\n",
    "Ensure that you are using the right k values and data files here"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "source": [
    "X_train_total = df_local_features_train_copy.copy() #df_local_features_train #df_local_features_train_copy\n",
    "y_train_total = df_local_names #df_local_names#df_local_names_copy\n",
    "rndm_ste = 2016\n",
    "k_best = SelectKBest(f_classif, k=38)\n",
    "fit = k_best.fit(X_train_total, y_train_total)\n",
    "X_new = fit.transform(X_train_total)\n",
    " #X_train_set.values #\n",
    "X_train_total = pd.DataFrame(X_new.copy())\n",
    "\n",
    "rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "     n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700)   \n",
    "sme = SMOTE(random_state=2016, sampling_strategy=0.7, n_jobs=-1, k_neighbors=12)\n",
    "X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train_total, y_train_total)\n",
    "rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "\n",
    "total_train_test = pd.DataFrame()\n",
    "total_train_test['Protein Name'] = df_local_protein_names_classify_copy\n",
    "# pd.DataFrame(fit.transform(df_local_features_classify_copy))\n",
    "# total_train_test['In Corona Probability'] = rfc.predict_proba(df_local_features_classify_copy)[:, 1]\n",
    "\n",
    "print(len(k_best.get_support()), df_local_features_train_copy.shape)\n",
    "total_train_test['In Corona Probability'] = rfc.predict_proba(pd.DataFrame(df_local_features_classify_copy.loc[:,k_best.get_support()]))[:, 1]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "91 (174, 91)\n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "# print weights for table s2\n",
    "pd.Series(rfc.feature_importances_,index=list(df_local_features_train_copy.columns)).sort_values(ascending=False).to_excel('revisions_data_2/table_s2_weights.xlsx')"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "source": [
    "# for full dataset testing scoire\n",
    "y_test_score = y_test_test \n",
    "X_score = df_local_features_classify_copy.loc[:,k_best.get_support()]\n",
    "\n",
    "results_dict = {'AUC':metrics.roc_auc_score(y_test_score, rfc.predict_proba(X_score)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_score, y_test_score), 'Recall':recall_score(y_test_score, rfc.predict(X_score)), \n",
    "        'Precision':precision_score(y_test_score, rfc.predict(X_score)), 'F1':f1_score(y_test_score, rfc.predict(X_score))}\n",
    "\n",
    "pprint(results_dict)\n",
    "count_proxy  = total_train_test.copy()\n",
    "count_proxy['In Corona'] = count_proxy['In Corona Probability'] >= 0.5 \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'AUC': 0.8479166666666667,\n",
      " 'Accuracy': 0.8064516129032258,\n",
      " 'F1': 0.7777777777777778,\n",
      " 'Precision': 0.9545454545454546,\n",
      " 'Recall': 0.65625}\n",
      "                                         Protein Name  In Corona Probability  \\\n",
      "1            Transthyretin (ATTR) (Prealbumin) (TBPA)               0.841727   \n",
      "3   Prostaglandin-H2 D-isomerase (EC 5.3.99.2) (Be...               0.764176   \n",
      "8   Immunoglobulin heavy constant gamma 1 (Ig gamm...               0.764373   \n",
      "11                   Hemopexin (Beta-1B-glycoprotein)               0.600140   \n",
      "12                           Apolipoprotein E (Apo-E)               0.810779   \n",
      "14  Apolipoprotein A-I (Apo-AI) (ApoA-I) (Apolipop...               0.788691   \n",
      "15  Clusterin (Aging-associated gene 4 protein) (A...               0.882423   \n",
      "17  Gelsolin (AGEL) (Actin-depolymerizing factor) ...               0.524507   \n",
      "19  Complement C3 (C3 and PZP-like alpha-2-macrogl...               0.875599   \n",
      "40                         Collagen alpha-2(XI) chain               0.812025   \n",
      "42  Kininogen-1 (Alpha-2-thiol proteinase inhibito...               0.891528   \n",
      "44  Histidine-rich glycoprotein (Histidine-proline...               0.938621   \n",
      "45  Vitronectin (VN) (S-protein) (Serum-spreading ...               0.977106   \n",
      "46  Major prion protein (PrP) (ASCR) (PrP27-30) (P...               0.878824   \n",
      "47              Complement C1q subcomponent subunit B               0.826776   \n",
      "48                   Complement factor H (H factor 1)               0.944314   \n",
      "49  Galectin-3-binding protein (Basement membrane ...               0.878971   \n",
      "50  Fibrinogen beta chain [Cleaved into: Fibrinope...               0.830651   \n",
      "51  Complement C1s subcomponent (EC 3.4.21.42) (C1...               0.641412   \n",
      "53  Fibrinogen alpha chain [Cleaved into: Fibrinop...               0.939227   \n",
      "55  EGF-containing fibulin-like extracellular matr...               0.815406   \n",
      "56  Prothrombin (EC 3.4.21.5) (Coagulation factor ...               0.638201   \n",
      "\n",
      "    In Corona  \n",
      "1        True  \n",
      "3        True  \n",
      "8        True  \n",
      "11       True  \n",
      "12       True  \n",
      "14       True  \n",
      "15       True  \n",
      "17       True  \n",
      "19       True  \n",
      "40       True  \n",
      "42       True  \n",
      "44       True  \n",
      "45       True  \n",
      "46       True  \n",
      "47       True  \n",
      "48       True  \n",
      "49       True  \n",
      "50       True  \n",
      "51       True  \n",
      "53       True  \n",
      "55       True  \n",
      "56       True  \n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "source": [
    "# display results\n",
    "total_train_test "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protein Name</th>\n",
       "      <th>In Corona Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transgelin (22 kDa actin-binding protein) (Pro...</td>\n",
       "      <td>0.549758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAR DNA-binding protein 43 (TDP-43)</td>\n",
       "      <td>0.535928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CD44 antigen (CDw44) (Epican) (Extracellular m...</td>\n",
       "      <td>0.588761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lysozyme C (EC 3.2.1.17) (1,4-beta-N-acetylmur...</td>\n",
       "      <td>0.352303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L-lactate dehydrogenase A chain (LDH-A) (EC 1....</td>\n",
       "      <td>0.247507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ribonuclease pancreatic (EC 4.6.1.18) (HP-RNas...</td>\n",
       "      <td>0.301310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Glutathione S-transferase (EC 2.5.1.18) (PfGST)</td>\n",
       "      <td>0.119809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Syntenin-1 (Melanoma differentiation-associate...</td>\n",
       "      <td>0.329141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Protein Name  In Corona Probability\n",
       "0  Transgelin (22 kDa actin-binding protein) (Pro...               0.549758\n",
       "1                TAR DNA-binding protein 43 (TDP-43)               0.535928\n",
       "2  CD44 antigen (CDw44) (Epican) (Extracellular m...               0.588761\n",
       "3  Lysozyme C (EC 3.2.1.17) (1,4-beta-N-acetylmur...               0.352303\n",
       "4  L-lactate dehydrogenase A chain (LDH-A) (EC 1....               0.247507\n",
       "5  Ribonuclease pancreatic (EC 4.6.1.18) (HP-RNas...               0.301310\n",
       "6    Glutathione S-transferase (EC 2.5.1.18) (PfGST)               0.119809\n",
       "7  Syntenin-1 (Melanoma differentiation-associate...               0.329141"
      ]
     },
     "metadata": {},
     "execution_count": 251
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Writes all Prediction Data (Including Revolving Predictions Data)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('2021_08_08_predictions_selected_proteins_gt6.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "\n",
    "predictions.to_excel(writer, sheet_name='Round Based Prediction')\n",
    "metrics_frame.to_excel(writer, sheet_name='Classifier Round Metrics')\n",
    "protein_avg_predictions.to_excel(writer, sheet_name='Protein Average Predictions')\n",
    "overall_probability_accuracy.to_excel(writer, sheet_name='Overall Probability Accuracy')\n",
    "total_train_test.to_excel(writer, sheet_name='Total Set Used in Prediction')\n",
    "\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_proxy  = total_train_test.copy()\n",
    "count_proxy['In Corona'] = count_proxy['In Corona Probability'] >= 0.5 \n",
    "print(count_proxy['In Corona'].sum())"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Add In Importance Test (Can take a very long time to run)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k_readouts ={}\n",
    "first_loop = True\n",
    "first_feat=True\n",
    "kselect_params={}\n",
    "trials = 100\n",
    "\n",
    "\n",
    "rndm_ste=2016\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=rndm_ste)\n",
    "\n",
    "feature_imp = pd.DataFrame(columns=list(df_local_features_train_copy.columns))\n",
    "for k_feat in range(45, 51, 1):   \n",
    "    i = 0\n",
    "\n",
    "    k_best = SelectKBest(f_classif, k=k_feat)\n",
    "    fit = k_best.fit(df_local_features_train_copy.copy(), df_local_names.copy())\n",
    "    X_new = fit.transform(df_local_features_train_copy.copy()) #X_train_set.values #\n",
    "    df_local_features_train = pd.DataFrame(X_new.copy())\n",
    "    \n",
    "    init_scores_rfc = True\n",
    "    first_roc_rfc = True\n",
    "    # kselect_params[str(k_feat)]=k_best.get_support()\n",
    "    for train_index, test_index in sss.split(df_local_features_train, df_local_names):\n",
    "    \n",
    "        X_train = df_local_features_train.iloc[train_index]\n",
    "        X_test = df_local_features_train.iloc[test_index]\n",
    "        y_train = df_local_names.iloc[train_index]\n",
    "        y_test = df_local_names.iloc[test_index]    \n",
    "        #### END COMMENT OUT \n",
    "        if init_scores_rfc:  # use this to record data for ROC curves -- Some may be moved to outside the loop\n",
    "            y_score_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            y_true_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            tpr_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            fpr_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            score_rfc = np.zeros(trials)\n",
    "            score_svm = np.zeros(trials)\n",
    "            auc_data_rfc = np.zeros(trials)\n",
    "            f1_rfc = np.zeros(trials)\n",
    "            fbeta_rfc = np.zeros(trials)\n",
    "            recall_rfc = np.zeros(trials)\n",
    "            precision_rfc = np.zeros(trials)\n",
    "            log_loss_rfc = np.zeros(trials)\n",
    "            features_left = np.zeros(trials)\n",
    "            \n",
    "            init_scores_rfc = False # Don't Come Back Here\n",
    "        \n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "                                    n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700)\n",
    "                                #min_samples_split=4, min_samples_leaf= 2, max_features= 'log2', max_depth = 10)    ## max_leaf_nodes=20, \n",
    "        \n",
    "        \n",
    "        sme = SMOTE(random_state=rndm_ste, sampling_strategy=.7, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "        \n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "        # Basic Predictions\n",
    "        y_pred_test = rfc.predict(X_train_oversampled) \n",
    "        y_pred_train = rfc.predict(X_train)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Calculate Metrics\n",
    "        auc_data_rfc[i] = metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "        score_rfc[i] =  rfc.score(X_test, y_test)\n",
    "\n",
    "        f1_rfc[i] = f1_score(y_test, rfc.predict(X_test))\n",
    "        fbeta_rfc[i] = fbeta_score(y_test, rfc.predict(X_test), beta=0.5)\n",
    "        recall_rfc[i] = recall_score(y_test, rfc.predict(X_test))\n",
    "        precision_rfc[i] = precision_score(y_test, rfc.predict(X_test))\n",
    "        log_loss_rfc[i] = log_loss(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "        fpr_current_list, tpr_current_list, _ = metrics.roc_curve(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "\n",
    "\n",
    "        if first_roc_rfc:  # Initialize \n",
    "            fpr_array_rfc = fpr_current_list\n",
    "            tpr_array_rfc = tpr_current_list\n",
    "            first_roc_rfc = False  # Don't Come back Here\n",
    "            \n",
    "        else:\n",
    "            fpr_array_rfc = np.concatenate((fpr_array_rfc, fpr_current_list))\n",
    "            tpr_array_rfc = np.concatenate((tpr_array_rfc, tpr_current_list))\n",
    "#         i+=1\n",
    "        column_list = list(scaled_df.columns)\n",
    "        \n",
    "        if first_feat:\n",
    "            feat_revolve = pd.DataFrame(rfc.feature_importances_,index=[column_list[i] for i in np.nonzero(k_best.get_support())[0]]).transpose()\n",
    "            feat_revolve['Features'] = k_feat\n",
    "            feature_imp = feat_revolve.copy()\n",
    "            first_feat = False\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            feat_revolve = pd.DataFrame(rfc.feature_importances_,index=[column_list[i] for i in np.nonzero(k_best.get_support())[0]]).transpose()\n",
    "            feat_revolve['Features'] = k_feat\n",
    "            feature_imp = feature_imp.append(feat_revolve.copy(), ignore_index=True)\n",
    "\n",
    "        kselect_params[str(k_feat)]=k_best.get_support()\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    k_readouts[str(k_feat)] = {'Accuracy rfc': score_rfc.mean(),\n",
    "                               'Accuracy rfc ci': confidence_interval(score_rfc),\n",
    "                               'ROC Score rfc': auc_data_rfc.mean(), \n",
    "                               'ROC Score rfc ci': confidence_interval(auc_data_rfc),\n",
    "                               'Precision rfc': precision_rfc.mean(),\n",
    "                               'Precision rfc ci': confidence_interval(precision_rfc),\n",
    "                               'Recall rfc': recall_rfc.mean(),\n",
    "                               'Recall rfc ci': confidence_interval(recall_rfc),\n",
    "                               'Accuracy svm': score_svm.mean(),\n",
    "                               \"Accuracy svm ci\" : confidence_interval(score_svm)}\n",
    "    print(f'K Criteria: {k_feat}\\nAccuracy: {score_rfc.mean():.03f} +/- {confidence_interval(score_rfc):.03f}\\nROC Score: {auc_data_rfc.mean():.03f} +/- {confidence_interval(auc_data_rfc):.03f} \\n Precision: {precision_rfc.mean():.03f} \\nRecall: {recall_rfc.mean():.03f}') #Features Left: {features_left.mean():.1f} +/- {confidence_interval(features_left):.02f}')"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "first_key = True\n",
    "for key in k_readouts.keys():\n",
    "    \n",
    "    if first_key:\n",
    "        k_feats_data = pd.DataFrame.from_dict(data=k_readouts[key],orient='index').transpose()\n",
    "        k_feats_data['K Features'] = int(key)\n",
    "        first_key=False\n",
    "        \n",
    "    else: \n",
    "        k_feats_data_rotating = pd.DataFrame.from_dict(data=k_readouts[key],orient='index').transpose()\n",
    "        k_feats_data_rotating['K Features'] = int(key)\n",
    "        k_feats_data = k_feats_data.append(k_feats_data_rotating, ignore_index=True)\n",
    "        \n",
    "k_feats_data = k_feats_data.set_index('K Features')\n",
    "k_feats_data.to_excel('data_for_feature_add_in_figure_example.xlsx')        "
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "## prints features in order of importance (use all features for best results)\n",
    "first_pass = True\n",
    "for key, value in kselect_params.items():\n",
    "    if first_pass:\n",
    "        print(key, list(df_local_features_train_copy.loc[:,value].columns)[0])\n",
    "        prev_list = list(df_local_features_train_copy.loc[:,value].columns)\n",
    "        first_pass = False\n",
    "    else: \n",
    "        for i in list(df_local_features_train_copy.loc[:,value].columns):\n",
    "            if i not in prev_list:\n",
    "                print(key, i)\n",
    "                prev_list = list(df_local_features_train_copy.loc[:,value].columns)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}