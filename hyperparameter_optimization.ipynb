{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports --- All of this may not be vital\n",
    "\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import plot_roc_curve, log_loss, f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "# from sklearn.metrics import log_loss, f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "import urllib.request, json\n",
    "from skimage.filters import threshold_otsu\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "# Homemade functions required\n",
    "from data_prep_functions import *\n",
    "from interpro_scraping import interpro_scraping_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(174, 90)\n"
     ]
    }
   ],
   "source": [
    "### import data used to train classifiers ###\n",
    "\n",
    "plasma_total_data_names = pd.read_excel(\"data/\"+'gt15_plasma_features_names_biopy.xlsx', header=0, index_col=0)\n",
    "csf_total_data_names = pd.read_excel(\"data/\"+'gt15_csf_features_names_biopy.xlsx', header=0,index_col=0)\n",
    "\n",
    "## sort into names and features\n",
    "features_plasma = plasma_total_data_names.copy()\n",
    "features_plasma = features_plasma.drop(['Corona'], axis=1)\n",
    "names_plasma = plasma_total_data_names['Corona'].copy()\n",
    "\n",
    "features_csf = csf_total_data_names.copy()\n",
    "features_csf = features_csf.drop(['Corona'], axis=1) \n",
    "names_csf = csf_total_data_names['Corona'].copy()\n",
    "\n",
    "### create a merged set\n",
    "features_plasma_labeled = features_plasma.copy()\n",
    "features_csf_labeled = features_csf.copy()\n",
    "\n",
    "features_plasma_labeled['phase_plasma'] = 1\n",
    "features_csf_labeled['phase_plasma'] = 0\n",
    "\n",
    "features_merged = features_plasma_labeled.append(features_csf_labeled, ignore_index=True)\n",
    "names_merged = names_plasma.append(names_csf, ignore_index=True)\n",
    "\n",
    "# set with no phase labeling names are identical to names merged\n",
    "features_merged_naive = features_merged.drop(['phase_plasma'], axis=1)\n",
    "\n",
    "# print(plasma_total_data_names.shape, csf_total_data_names.shape, features_test.shape) ## in case you need to see shapes\n",
    "\n",
    "## there is a known error here, sometimes there is an Unnamed column just drop it code is available in a \n",
    "#lower cell (scaling cell), its a holdover from two merged sets\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "total_data = features_merged_naive.copy()  ## for a regular netsurfp included case\n",
    "# total_data = subset_features.copy() ### for a subset case --- use this one\n",
    "total_data = total_data.fillna(0)\n",
    "total_data_with_names = total_data.copy()\n",
    "total_data = total_data.drop(['Protein names'], axis=1)\n",
    "scaler = scaler.fit(total_data)\n",
    "scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "print(scaled_df.shape)\n",
    "\n",
    "# total_data_labeled = features_merged.copy()\n",
    "# total_data_labeled = total_data_labeled.fillna(0)\n",
    "# total_data_labeled = total_data_labeled.drop(['Protein names'], axis=1)\n",
    "# scaled_df_total = pd.DataFrame(scaler.transform(total_data_labeled), columns=total_data_labeled.columns)\n",
    "\n",
    "scaled_df_phase = scaled_df.copy()\n",
    "scaled_df_phase['phase_plasma'] = features_merged['phase_plasma'].copy()\n",
    "\n",
    "plasma_data = scaled_df_phase[scaled_df_phase.phase_plasma==1]\n",
    "plasma_data = plasma_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_plasma = plasma_data #pd.DataFrame(scaler.transform(plasma_data), columns=plasma_data.columns)\n",
    "\n",
    "csf_data = scaled_df_phase[scaled_df_phase.phase_plasma==0]\n",
    "csf_data = csf_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_csf = csf_data #pd.DataFrame(scaler.transform(csf_data), columns=csf_data.columns)\n",
    "\n",
    "#### UNCOMMENT this section for a REGULAR RUN\n",
    "# #features = features_merged_naive.copy()  # change the dataframe that you want to use here\n",
    "# features_test = features_test.fillna(0)\n",
    "# features_test_names = features_test.copy()\n",
    "# features_test = features_test.drop(['Protein names'], axis=1)\n",
    "# scaled_test_df = pd.DataFrame(scaler.transform(features_test), columns=features_test.columns)\n",
    "\n",
    "\n",
    "scaled_df = scaled_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "# scaled_test_df = scaled_test_df.drop(['Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   frac_aa_A  frac_aa_C  frac_aa_D  frac_aa_E  frac_aa_F  frac_aa_G  \\\n",
       "0   0.457240   0.561143   0.300342   0.471036   0.746791   0.010886   \n",
       "1   0.268538   0.288588   0.318724   0.247340   0.180208   0.252494   \n",
       "2   0.214455   0.273754   0.207921   0.268716   0.445427   0.081323   \n",
       "3   0.048290   0.266288   0.153181   0.191396   0.339209   0.156703   \n",
       "4   0.354733   0.559535   0.340318   0.239531   0.486404   0.244341   \n",
       "\n",
       "   frac_aa_H  frac_aa_I  frac_aa_K  frac_aa_L  ...  \\\n",
       "0   0.208986   0.186978   0.811229   0.522934  ...   \n",
       "1   0.254702   0.560934   0.701237   0.262321  ...   \n",
       "2   0.148683   0.118245   0.599058   0.247915  ...   \n",
       "3   0.216942   0.191700   0.689105   0.284388  ...   \n",
       "4   0.216528   0.290021   0.673440   0.414400  ...   \n",
       "\n",
       "   fraction_exposed_exposed_S  fraction_exposed_exposed_T  \\\n",
       "0                    0.198797                    0.399849   \n",
       "1                    0.202440                    0.131174   \n",
       "2                    1.000000                    0.589823   \n",
       "3                    0.671720                    0.637246   \n",
       "4                    0.268621                    0.256670   \n",
       "\n",
       "   fraction_exposed_exposed_V  fraction_exposed_exposed_W  \\\n",
       "0                    0.299104                    0.072222   \n",
       "1                    0.545589                    0.103610   \n",
       "2                    0.365223                    0.000000   \n",
       "3                    0.187414                    0.108174   \n",
       "4                    0.189878                    0.000000   \n",
       "\n",
       "   fraction_exposed_exposed_Y  nsp_secondary_structure_coil  \\\n",
       "0                    0.250980                      0.163934   \n",
       "1                    0.810127                      0.483607   \n",
       "2                    0.288288                      0.277518   \n",
       "3                    0.375918                      0.396956   \n",
       "4                    0.126233                      0.384075   \n",
       "\n",
       "   nsp_secondary_structure_sheet  nsp_secondary_structure_helix  \\\n",
       "0                       0.000000                       0.836066   \n",
       "1                       0.716763                       0.080796   \n",
       "2                       0.990366                       0.120609   \n",
       "3                       0.899807                       0.056206   \n",
       "4                       0.358382                       0.398126   \n",
       "\n",
       "   nsp_disordered  asa_sum_normalized  \n",
       "0           0.038            0.164590  \n",
       "1           0.074            0.201285  \n",
       "2           0.009            0.293308  \n",
       "3           0.012            0.238334  \n",
       "4           0.030            0.101516  \n",
       "\n",
       "[5 rows x 89 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frac_aa_A</th>\n      <th>frac_aa_C</th>\n      <th>frac_aa_D</th>\n      <th>frac_aa_E</th>\n      <th>frac_aa_F</th>\n      <th>frac_aa_G</th>\n      <th>frac_aa_H</th>\n      <th>frac_aa_I</th>\n      <th>frac_aa_K</th>\n      <th>frac_aa_L</th>\n      <th>...</th>\n      <th>fraction_exposed_exposed_S</th>\n      <th>fraction_exposed_exposed_T</th>\n      <th>fraction_exposed_exposed_V</th>\n      <th>fraction_exposed_exposed_W</th>\n      <th>fraction_exposed_exposed_Y</th>\n      <th>nsp_secondary_structure_coil</th>\n      <th>nsp_secondary_structure_sheet</th>\n      <th>nsp_secondary_structure_helix</th>\n      <th>nsp_disordered</th>\n      <th>asa_sum_normalized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.457240</td>\n      <td>0.561143</td>\n      <td>0.300342</td>\n      <td>0.471036</td>\n      <td>0.746791</td>\n      <td>0.010886</td>\n      <td>0.208986</td>\n      <td>0.186978</td>\n      <td>0.811229</td>\n      <td>0.522934</td>\n      <td>...</td>\n      <td>0.198797</td>\n      <td>0.399849</td>\n      <td>0.299104</td>\n      <td>0.072222</td>\n      <td>0.250980</td>\n      <td>0.163934</td>\n      <td>0.000000</td>\n      <td>0.836066</td>\n      <td>0.038</td>\n      <td>0.164590</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.268538</td>\n      <td>0.288588</td>\n      <td>0.318724</td>\n      <td>0.247340</td>\n      <td>0.180208</td>\n      <td>0.252494</td>\n      <td>0.254702</td>\n      <td>0.560934</td>\n      <td>0.701237</td>\n      <td>0.262321</td>\n      <td>...</td>\n      <td>0.202440</td>\n      <td>0.131174</td>\n      <td>0.545589</td>\n      <td>0.103610</td>\n      <td>0.810127</td>\n      <td>0.483607</td>\n      <td>0.716763</td>\n      <td>0.080796</td>\n      <td>0.074</td>\n      <td>0.201285</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.214455</td>\n      <td>0.273754</td>\n      <td>0.207921</td>\n      <td>0.268716</td>\n      <td>0.445427</td>\n      <td>0.081323</td>\n      <td>0.148683</td>\n      <td>0.118245</td>\n      <td>0.599058</td>\n      <td>0.247915</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.589823</td>\n      <td>0.365223</td>\n      <td>0.000000</td>\n      <td>0.288288</td>\n      <td>0.277518</td>\n      <td>0.990366</td>\n      <td>0.120609</td>\n      <td>0.009</td>\n      <td>0.293308</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.048290</td>\n      <td>0.266288</td>\n      <td>0.153181</td>\n      <td>0.191396</td>\n      <td>0.339209</td>\n      <td>0.156703</td>\n      <td>0.216942</td>\n      <td>0.191700</td>\n      <td>0.689105</td>\n      <td>0.284388</td>\n      <td>...</td>\n      <td>0.671720</td>\n      <td>0.637246</td>\n      <td>0.187414</td>\n      <td>0.108174</td>\n      <td>0.375918</td>\n      <td>0.396956</td>\n      <td>0.899807</td>\n      <td>0.056206</td>\n      <td>0.012</td>\n      <td>0.238334</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.354733</td>\n      <td>0.559535</td>\n      <td>0.340318</td>\n      <td>0.239531</td>\n      <td>0.486404</td>\n      <td>0.244341</td>\n      <td>0.216528</td>\n      <td>0.290021</td>\n      <td>0.673440</td>\n      <td>0.414400</td>\n      <td>...</td>\n      <td>0.268621</td>\n      <td>0.256670</td>\n      <td>0.189878</td>\n      <td>0.000000</td>\n      <td>0.126233</td>\n      <td>0.384075</td>\n      <td>0.358382</td>\n      <td>0.398126</td>\n      <td>0.030</td>\n      <td>0.101516</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 89 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 25 folds for each of 144 candidates, totalling 3600 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   37.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3600 out of 3600 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=25, random_state=2020, test_size=0.1,\n",
       "            train_size=None),\n",
       "             estimator=Pipeline(steps=[('sm',\n",
       "                                        SMOTE(n_jobs=-1, random_state=2020,\n",
       "                                              sampling_strategy=1)),\n",
       "                                       ('rf',\n",
       "                                        RandomForestClassifier(random_state=2020))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__criterion': ['entropy', 'gini'],\n",
       "                         'rf__n_estimators': [100, 500, 1000],\n",
       "                         'sm__k_neighbors': [5, 10, 15, 20],\n",
       "                         'sm__sampling_strategy': [0.5, 0.6, 0.7, 0.8, 0.9, 1]},\n",
       "             scoring='precision', verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "params = {'sm__k_neighbors':[5, 10, 15, 20],\n",
    "          'sm__sampling_strategy': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n",
    "          'rf__criterion': ['entropy', 'gini'],\n",
    "          'rf__n_estimators': [100, 500, 1000, 1500]\n",
    "         }\n",
    "         #           'rf__max_depth': [5, 10, 15, 20, None],\n",
    "        #   'rf__max_features': ['sqrt', 'log2'],\n",
    "\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=25, test_size=0.1, random_state=2020)\n",
    "rf = RandomForestClassifier()\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "smote = SMOTE(sampling_strategy=1, random_state=2020, n_jobs=-1)\n",
    "clf_rf = RandomForestClassifier(n_estimators=100, random_state=2020)\n",
    "    \n",
    "pipeline = Pipeline([\n",
    "        ('sm', smote),\n",
    "        ('rf', clf_rf)])\n",
    "    \n",
    "rf_random = GridSearchCV(estimator = pipeline, param_grid = params, cv = sss, n_jobs = -1, scoring='precision', verbose=1)\n",
    "\n",
    "rf_random.fit(scaled_df, names_merged)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'rf__criterion': 'gini', 'rf__n_estimators': 1000, 'sm__k_neighbors': 20, 'sm__sampling_strategy': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}